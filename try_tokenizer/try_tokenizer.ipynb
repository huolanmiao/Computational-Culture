{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[258, 100, 258, 97, 99]\n",
      "['aaab', 'd', 'aaab', 'a', 'c']\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Toy example\n",
    "from bpe import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "text = \"aaabdaaabac\"\n",
    "tokenizer.train(text, 256 + 3) # 256 are the byte tokens, then do 3 merges\n",
    "print(tokenizer.encode(text))\n",
    "tokens = tokenizer.encode(text)\n",
    "# [258, 100, 258, 97, 99]\n",
    "word = [tokenizer.decode([i]) for i in tokens]\n",
    "print(word)\n",
    "print(text == tokenizer.decode(tokenizer.encode(text)))\n",
    "# aaabdaaabac\n",
    "# tokenizer.save(\"toy\")\n",
    "# writes two files: toy.model (for loading) and toy.vocab (for viewing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 很慢，不要运行，结果在train.txt中\n",
    "# 验证Tokenizer在encoder再decode之后与原文一致\n",
    "from bpe import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.load(\"base_tokenizer.model\")\n",
    "with open('./base.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "print(f\"encode and decode match: {text == tokenizer.decode(tokenizer.encode(text))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\nlp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\anaconda\\envs\\nlp\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\li862\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 10 tokens of text1 [28938, 249, 36310, 162, 249, 108, 171, 120, 248, 27764]\n",
      "length: 201\n",
      "tokens: ['�', '�', '子', '�', '�', '�', '�', '�', '�', '�', '�', '不', '�', '�', '�', '�', '�', '�', '。', '�', '�', '�', '、', '�', '�', '之', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '、', '�', '�', '�', '�', '�', '之', '�', '�', '�', '�', '�', '�', '�', '�', '。', '�', '�', '�', '�', '中', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '��', '�', '�', '中', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '不', '�', '�', '�', '�', '�', '者', '�', '�', '�', '�', '�', '�', '使', '之', '�', '�', '�', '�', '。', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '子', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '。']\n",
      "first 10 tokens of text2 [36310, 162, 249, 108, 171, 120, 248, 27764, 116, 32003]\n",
      "length: 246\n",
      "tokens: ['子', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '��', '�', '�', '�', '之', '�', '�', '�', '不', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '子', '者', '�', '�', '�', '�', '�', '子', '之', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '��', '子', '�', '�', '。', '王', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '��', '者', '�', '�', '�', '�', '�', '者', '�', '�', '�', '��', '�', '�', '�', '�', '�', '之', '。', '�', '�', '�', '�', '�', '�', '�', '�', '��', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '。', '�', '�', '�', '�', '�', '�', '�', '�', '�', '方', '�', '�', '�', '�', '�', '不', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '。', '人', '不', '�', '�', '�', '�', '�', '不', '�', '�', '�', '�', '�', '不', '�', '�', '�', '�', '子', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', '。', '�', '�', '人', '�', '�', '�', '�', '不', '�', '�', '�', '�', '�', '�', '�', '�', '子', '不', '�', '�', '�', '�', '。']\n"
     ]
    }
   ],
   "source": [
    "# 使用gpt2 tokenizer encode 示例句子\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "# 加载 GPT-2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# 测试 gpt2 tokenizer\n",
    "text1 = \"君子曰：學不可以已。青、取之於藍而青於藍；冰、水為之而寒於水。木直中繩，輮以為輪，其曲中規，雖有槁暴，不復挺者，輮使之然也。故木受繩則直，金就礪則利，君子博學而日參省乎己，則智明而行無過矣。\"\n",
    "encoded_input = tokenizer(text1)['input_ids']\n",
    "print(\"first 10 tokens of text1\", encoded_input[:10])\n",
    "print(\"length:\", len(encoded_input))\n",
    "tokens = [tokenizer.decode([i]) for i in encoded_input]\n",
    "print(f\"tokens: {tokens}\")\n",
    "\n",
    "text2 =\"子曰：學而時習之，不亦悦乎？馬融曰：子者，男子之通稱，謂孔子也。王肅曰：時者，學者以時誦習之。誦習以時，學無廢業，所以爲悦懌也。有朋自逺方来，不亦樂乎？苞氏曰：同門曰朋也。人不知而不愠，不亦君子乎！愠，怒也。凡人有所不知，君子不愠也。\"\n",
    "encoded_input = tokenizer(text2)['input_ids']\n",
    "print(\"first 10 tokens of text2\", encoded_input[:10])\n",
    "print(\"length:\", len(encoded_input))\n",
    "tokens = [tokenizer.decode([i]) for i in encoded_input]\n",
    "print(f\"tokens: {tokens}\")\n",
    "# 出现特殊字符的原因是，一些byte pair由于在gpt2的繁体古文语料太少，而没有充分的合并成有意义的词语（合并为对应为词语unicode编码）\n",
    "# 而在大模型训练和推理的时候，我们所说的token指的就是这些合并后的字节在整个词表中的索引\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "君\n",
      "子\n",
      "曰\n",
      "：\n"
     ]
    }
   ],
   "source": [
    "# 使用gpt2 tokenizer encode 示例句子\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "# 加载 GPT-2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print(tokenizer.decode([28938, 249]))\n",
    "print(tokenizer.decode([36310]))\n",
    "print(tokenizer.decode([162, 249, 108]))\n",
    "print(tokenizer.decode([171, 120, 248]))\n",
    "# 以下可以更清晰的看出tokenizer给出的编码，实际上是多个token对应中文里一个字（甚至冒号）的情况\n",
    "# 这样的编码明显是不适宜于理解中文文本的，其原因在于gpt2的训练语料不够丰富"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 10 tokens of text1 [1007, 326, 428, 621, 274, 497, 259, 363, 146, 358]\n",
      "length: 108\n",
      "tokens: ['君子', '曰：', '學', '不可', '以', '已', '。', '�', '�', '、', '取', '之', '於', '�', '�', '而', '�', '�', '於', '�', '�', '；', '�', '�', '、', '水', '為', '之', '而', '�', '�', '於', '水', '。', '木', '直', '中', '�', '�', '，�', '�', '�', '以為', '�', '�', '，其', '�', '�', '中', '�', '�', '，雖', '有', '�', '�', '�', '�', '�', '，不', '復', '�', '�', '者', '，�', '�', '�', '使', '之', '然', '也。', '故', '木', '受', '�', '�', '則', '直', '，�', '�', '�', '�', '�', '�', '�', '則', '利', '，�', '�', '子', '�', '�', '學', '而', '日', '�', '�', '�', '�', '乎', '己', '，則', '智', '明', '而', '行', '無', '過', '矣。']\n",
      "first 10 tokens of text2 [305, 326, 428, 275, 411, 678, 146, 262, 392, 375]\n",
      "length: 116\n",
      "tokens: ['子', '曰：', '學', '而', '時', '�', '�', '之', '，不', '亦', '�', '�', '乎？', '馬', '�', '�', '�', '曰：', '子', '者，', '�', '�', '子之', '通', '稱', '，謂', '孔', '子', '也。', '王', '�', '�', '曰：', '時', '者', '，�', '�', '者', '以', '時', '�', '�', '�', '�', '之。', '�', '�', '�', '�', '以', '時', '，�', '�', '無', '�', '�', '�', '�', '，所', '以爲', '�', '�', '�', '�', '也。', '有', '�', '�', '自', '�', '�', '方', '�', '�', '，不', '亦', '樂', '乎？', '�', '�', '氏', '曰：', '同', '門', '曰', '�', '�', '也。', '人', '不知', '而不', '�', '�', '，不', '亦', '君子', '乎', '！', '�', '�', '，�', '�', '�', '也。', '凡', '人', '有', '所', '不知', '，�', '�', '子', '不', '�', '�', '也。']\n"
     ]
    }
   ],
   "source": [
    "# 使用我训练的 tokenizer encode 示例句子\n",
    "from bpe import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.load(\"base_tokenizer.model\")\n",
    "\n",
    "# 测试 base tokenizer\n",
    "text1 = \"君子曰：學不可以已。青、取之於藍而青於藍；冰、水為之而寒於水。木直中繩，輮以為輪，其曲中規，雖有槁暴，不復挺者，輮使之然也。故木受繩則直，金就礪則利，君子博學而日參省乎己，則智明而行無過矣。\"\n",
    "encoded_input = tokenizer.encode(text1)\n",
    "print(\"first 10 tokens of text1\", encoded_input[:10])\n",
    "print(\"length:\", len(encoded_input))\n",
    "tokens = [tokenizer.decode([i]) for i in encoded_input]\n",
    "print(f\"tokens: {tokens}\")\n",
    "\n",
    "text2 =\"子曰：學而時習之，不亦悦乎？馬融曰：子者，男子之通稱，謂孔子也。王肅曰：時者，學者以時誦習之。誦習以時，學無廢業，所以爲悦懌也。有朋自逺方来，不亦樂乎？苞氏曰：同門曰朋也。人不知而不愠，不亦君子乎！愠，怒也。凡人有所不知，君子不愠也。\"\n",
    "encoded_input = tokenizer.encode(text2)\n",
    "print(\"first 10 tokens of text2\", encoded_input[:10])\n",
    "print(\"length:\", len(encoded_input))\n",
    "tokens = [tokenizer.decode([i]) for i in encoded_input]\n",
    "print(f\"tokens: {tokens}\")\n",
    "# 可以看到针对base文件夹的中文繁体古文文本训练的tokenizer，能更好的捕捉语义信息\n",
    "# “君子”、“曰：”被合并成一个token"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
